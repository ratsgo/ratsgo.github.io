---
title: False Discovery Rate
category: Machine Learning
tag: Feature Selection
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

이번 글에서는 **변수선택(Feature Selection)** 기법으로 널리 쓰이고 있는 **False Discovery Rate(FDR)**에 대해 살펴보도록 하겠습니다. 이번 글 역시 고려대 김성범 교수님 강의를 정리하였음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## FDR의 문제 의식

제품의 '정상', '불량'을 판별하는 이진분류 문제를 푼다고 칩시다. (정상, 불량이든 긍정, 부정이든 어떤 것이든 상관없습니다) 우리가 가진 데이터의 독립변수는 $x_1, x_2, x_3$ 세 개이고 하나의 행(레코드)은 관측치 하나에 대응한다고 가정하겠습니다. 그렇다면 세 변수 가운데 정상, 불량을 판별하는 데 가장 중요한 변수는 무엇일까요?

<a href="http://imgur.com/e2T9Rnn"><img src="http://i.imgur.com/e2T9Rnn.png" width="400px" title="source: imgur.com" /></a>

위 예시에서 $x_1$은 정상, 불량 범주를 판별하는 데 전혀 도움이 안 될 것입니다. 반면 $x_2$는 상대적으로 중요할 겁니다. 정상 관측치에선 작은 값(평균 : 10)을, 불량 관측치에선 큰 값(200)을 갖기 때문입니다. $x_3$는 $x_1$보다는 중요한 변수지만 $x_3$보다는 덜 중요한 변수일 겁니다.

지금까지 말씀드린 내용을 이를 수식으로 나타내면 아래와 같습니다. 이 식에 따라 좌변을 계산하면 각각 0, 190, 1.6이 됩니다.


$$
\left| \overline { { x }_{ i,normal } } -\overline { { x }_{ i,abnormal } }  \right| \ge \tau
$$

그렇다면 $τ$가 얼마나 커야 유의미(=$x_i$가 중요변수)할까요? 이를 엄밀히 따져 보려면 통계학 기법을 써야 합니다. 다시 말해 **귀무가설(Null Hypothesis)**과 **대립가설(Alternative Hypothesis)**을 아래와 같이 설정하고, 변수마다 각각 **가설검정(Hypothesis Test)**을 실시해야 한다는 것이지요. 



$$
{ H }_{ 0 }\quad :\quad \left| \overline { { x }_{ i,normal } } -\overline { { x }_{ i,abnormal } }  \right| =0\\ { H }_{ 1 }\quad :\quad \left| \overline { { x }_{ i,normal } } -\overline { { x }_{ i,abnormal } }  \right| >0
$$


그런데 여기서 문제가 있습니다. 우리는 대개 변수가 100개를 훌쩍 넘는 다변량 데이터를 취급한다는 점입니다. 모든 가설을 동시에 검정하면 $x_i$가 중요하지 않은 변수($H_0$가 참)인데도 중요변수($H_0$를 기각)라는 결과가 나오는 경향이 있습니다. 이를 **다중성(Multiplicity)** 문제라고 합니다. 

개별 가설검정의 **유의수준(significance level)**이 0.01, 다시 말해 개별 가설검정의 **신뢰도**가 0.99인 상황에서 가설검정을 실시한다고 합시다. 유의수준이란 귀무가설 $H_0$가 참인데 잘못해서 기각할 확률(**제1종 오류**)의 최대값을 뜻합니다. 

그럼 변수가 100개인 다변량 데이터에 대해 변수선택을 하기 위해 100번의 가설검정을 실시한다면 100회 가설검정 전체의 신뢰도는 크게 낮아집니다. 다시 말해 중요하지 않은 변수인데도 중요변수라는 결과가 나올 가능성이 높아진다는 얘기입니다. 아래 표를 볼까요?




| 검정 횟수 |    검정 전체의 신뢰도     |
| :---: | :---------------: |
|   1   |   $0.99^1=0.99$   |
|   2   |   $0.99^2=0.98$   |
|  10   | $0.99^{10}=0.90$  |
|  100  | $0.99^{100}=0.37$ |



이를 그래프로 나타내면 아래와 같습니다.



<a href="http://imgur.com/wBYsEEw"><img src="http://i.imgur.com/wBYsEEw.png" width="300px" title="source: imgur.com" /></a>

FDR은 이러한 다중가설검정 문제를 해결하기 위해 제안된 방법입니다. 물론 개별 유의수준($α$)을 '목표값/변수개수'로 정하고 기존대로 가설검정을 수행하는 기법(Turkey)이 제시되기도 했습니다. 예컨대 전체 목표 유의수준이 0.01이고 변수개수가 10개라면 개별 유의수준을 0.001로 놓고 가설검정을 각각 실시하는 방식이죠. 그러나 이 방식은 중요한 변수($H_0$가 거짓)인데도 중요하지 않다($H_0$가 참)는 결론을 내는 경우가 많은 단점이 있다고 합니다. 



## FDR 절차

FDR은 제1종 오류를 최대한 적게 범하면서도 중요한 가설을 최대한 많이 찾을 수 있도록($H_0$ 기각) 한다는 점이 강점입니다. 검정해야할 다중가설(데이터의 변수 개수)이 $m$개일 때 FDR은 아래와 같이 정의됩니다.

|    구분     | $H_0$ 채택 | $H_0$ 기각 | Total |
| :-------: | :------: | :------: | :---: |
| $H_0$가 참  |   $U$    |   $V$    | $m_0$ |
| $H_0$가 거짓 |   $T$    |   $S$    | $m_1$ |
|   Total   |   $W$    |   $R$    |  $m$  |

$$
FDR=E\left[ \frac { V }{ V+S }  \right] =E\left[ \frac { V }{ R }  \right]
$$

위 수식에 정의된 FDR을 해석하면 이렇습니다. FDR은 기각된 귀무가설($H_0$) 가운데 잘못 기각된 가설이 차지하는 비율의 평균으로 사용자가 지정해주는 값입니다. 바꿔 말해 $m$회 가설검정 가운데 각 $H_0$이 참인데 기각된 비율, 즉 검정 전체 유의수준에 가까운 의미라는 뜻입니다. 실제로 FDR 제안자는 FDR을 $α$라고 표기하고 있습니다.

그럼 FDR 절차를 살펴볼까요? 검정해야할 다중가설, 즉 데이터 변수 개수가 15개라고 합시다. 목표로 하는 FDR 값을 0.05로 정해보겠습니다. 우선 각각의 가설에 대해 $p-value$를 구한 뒤 이를 오름차순으로 정렬하고, 순위(rank)를 매깁니다. 아래 표와 같습니다.

<a href="http://imgur.com/m40OJEw"><img src="http://i.imgur.com/m40OJEw.png" width="400px" title="source: imgur.com" /></a>

$p-value$는 $H_0$이 참일 확률을 의미하므로, $α$를 0.05로 놓고 기존대로 단일 가설검정을 실시했다면 1~9번 변수 가 중요변수라는 결과를 얻었을 겁니다. Turkey의 방식대로 '목표 유의수준/변수 개수=0.05/15'로 가설검정을 실시한다면, 그 값이 0.0033이므로 1번 변수만 중요하다는 결론을 내렸을 겁니다.

FDR 절차에서 $i$는 rank, $m$은 변수 개수, $α$는 사용자가 정한 FDR 값입니다. 예컨대 1번 변수는 $1/15$에 0.05를 곱해 계산합니다. 마찬가지로 2번 변수는 $2/15$에 0.05를 곱해 구합니다. 

이 값이 $p-value$보다 큰 변수 랭크의 최대값이 FDR 절차가 뽑은 중요변수의 개수가 됩니다. 위 표 기준으로 설명하자면 이렇습니다. 표 아래쪽부터 시작해 위쪽으로 훑어 가면서  $i/m*α$ 값이 $p-value$보다 큰 지 여부를 검토합니다. 처음으로 $p-value$ 값을 넘어서는 지점은 바로 4번 변수이네요. 따라서 1~4번 변수를 중요한 변수라고 결론을 내게 됩니다.

예제에서도 알 수 있듯 FDR은 아무런 처리를 하지 않은 다중가설검정에 비해 제1종 오류가 낮고(비중요 변수 배제), Turkey 방식에 비해 중요한 가설을 많이 찾아낸다는 장점이 있습니다.



## 반복샘플링 기반 FDR

지금까지 설명해드린 FDR 기법은 우리가 가진 데이터가 특정 **확률분포**를 따르고, 이로부터 적당한 방법을 취해 $p-value$를 계산할 수 있어야만 적용이 가능합니다. 하지만 데이터가 특정 분포를 따르지 않거나, 그 분포를 모른다고 할 때는 $p-value$ 계산이 어렵습니다. 반복샘플링 기반 FDR은 말 그대로 반복샘플링을 통해 분포에 대한 가정없이 $p-value$를 계산할 수 있는 기법입니다.
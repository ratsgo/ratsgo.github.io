---
title: 빈도수 세기의 놀라운 마법 Word2Vec, Glove, Fasttext 
category: From frequency to semantics
tag: embedding methods
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

안녕하세요. 이번 포스팅에서는 단어를 벡터화하는 **임베딩(embedding)** 방법론인 **Word2Vec, Glove, Fasttext**에 대해 알아보고자 합니다. 세 방법론은 대체 어떤 정보를 보존하면서 단어벡터를 만들기에 뛰어난 성능으로 유명세를 탄 것일까요? 저는 이번 포스팅에서 세 방법론이 크고 작은 차이점을 갖고 있지만 **단어 동시 등장 정보(word's of co-occurrence)**를 보존한다는 점에서 빈도수 기반의 방법론들과 본질적으로 같다는 점을 이야기해보려고 합니다. 저는 이 사실을 처음 깨닫고 나서 놀라움을 금치 못했었는데요. 자, 이제 시작해 볼까요.



## Word2vec의 단어벡터 학습방식

Word2Vec은 지난번 [포스트](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)에서 언급한 것처럼 단어를 벡터로 바꾸는 방법론입니다. 크게 **CBOW(Continous Bag of Words)**와 **Skip-Gram** 두 가지 방식이 있습니다. 전자는 주변에 있는 단어들을 가지고 중심에 있는 단어를 맞추는 방식이고, 후자는 중심에 있는 단어로 주변 단어를 예측하는 방법입니다. 예를 들어 보겠습니다.

> 나는 ______에 간다.

위 문장에 들어갈 수 있는 단어는 다양합니다. '학교'일 수도, '집'일 수도 있죠. '회사'일 수도 있습니다. 이렇듯 주변 단어를 가지고 중심에 있는 단어를 맞춤으로써 단어 벡터들을 만들어 내는 방법이 CBOW입니다. 반대로 아래처럼 '외나무다리' 앞뒤로 어떤 단어가 올지 예측하는 방법은 Skip-Gram입니다.

> ______  외나무다리  ______

'외나무다리' 앞에는 어떤 단어가 올 가능성이 높을까요? 아마도 '-는'이겠지요. 그 앞에는 '원수'가 올 가능성이 높고요. 뒤에는 어떤 단어가 올까요? '-에서'와 '만난다'가 될 가능성이 높겠네요. 우리가 학습시킬 말뭉치에서도 '외나무다리' 뒤에 '-에서', '만난다'는 표현이 등장했다고 칩시다. 그러면 Word2Vec은 '외나무다리'가 '-에서', '만난다'와 어떤 연관이 있다고 보고 이를 감안해서 단어를 벡터로 만들게 됩니다.

여기서 하나 같이 고민해볼 문제가 있습니다. '외나무다리'는 '원수'와 비슷한 표현(단어)라고 볼 수 있을까요? 정답이 없는 문제입니다만, 제 생각엔 그렇다고도, 그렇지 않다고도 볼 수 있을 것 같습니다. '외나무다리'는 '원수'와는 그 의미가 정확히 같지는 않지만, 동시에 같이 쓰이는 일종의 **연어(collocation)**로써 대체로 꺼리고 싫어하는 대상을 피할 수 없는 곳에서 공교롭게 만나게 됨을 비유적으로 말할 때 동시에 같이 쓰이기 때문입니다. 

Word2Vec은 **자연언어처리(Natural Language Processing)**의 대표적인 가정인 **[Distributional Hypothesis](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/)**(비슷한 맥락이나 위치에 등장하는 단어들은 그 의미도 유사하다)에 근거한 방법론입니다. Word2Vec은 어떤 방식으로 학습하고, 단어 벡터들을 만들어내는 걸까요? 최대한 직관적으로 설명해보려고 합니다. Word2Vec(Skip-Gram)은 아래 식을 최대화하는 걸 목표로 합니다.

$$p(o|c)=\frac { exp({ u }_{ o }^{ T }{ v }_{ c }) }{ \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c } } )}$$

식의 의미를 곱씹어 볼까요? o는 주변단어(surrounding word), c는 중심단어(context word)입니다. 다시 말해 p(o\|c)는 중심단어(c)가 주어졌을 때 주변단어(o)가 등장할 **조건부확률**을 뜻하는 것이죠. 이 식을 최대화하는 것은 중심단어로 주변단어를 잘 맞춘다(즉 위 예시에서 '외나무다리'라는 단어만으로 '원수'라는 표현이 등장한다는 사실을 맞추는 것)는 의미로 받아들이면 될 것 같습니다.

Word2Vec 연구진은 p(o\|c)을 위 식 우변과 같이 정의했습니다. u와 v는 단어벡터들입니다. 말하자면 '외나무다리'가 Word2Vec을 통해 벡터로 바뀐 것이 v_c이며, '원수'가 u_o인 셈이죠. (사실 엄밀히 얘기하면 u와 v는 다른 벡터들입니다, 하지만 학습 과정에서 u와 v는 수렴하며 u와 v를 아예 같은 벡터로 놓고 학습을 시킬 수도 있어서 설명에서 큰 차이를 두지 않았습니다) 

위 식 분모와 분자를 설명하기 전에 **코사인 유사도**를 설명하는 것이 좋겠습니다. 2차원 평면 위에 반지름이 1인 단위원이 있다고 칩시다. **[코사인(cosine)](https://ko.wikipedia.org/wiki/%EC%82%BC%EA%B0%81%ED%95%A8%EC%88%98)**의 정의에 의해 cos(θ)는 녹색 선의 길이와 같습니다. 아래 그림처럼 노란점을 꼭지점으로 하는 직각삼각형의 빗변의 길이는 단위원 반지름인 1이기 때문이죠. 아울러 코사인과 **내적(inner product)** 사이의 관계에 의해 단위원 위에 있는 벡터들끼리의 내적은 cos(θ)와 같습니다.

![cosine similarity](http://i.imgur.com/yL4dlAu.png)

예컨대 빨간점 위치에 있는 벡터(A)와 노란점 위치에 있는 벡터(B)가 있다고 합시다. B가 A에 정확히 포개어져 있을 때(θ=0도) cos(θ)는 1입니다. 녹색선의 길이가 반지름과 일치하기 때문입니다. A는 고정한 채 B가 y축 상단으로 옮겨간다(θ가 0도에서 90도로 증가)고 칩시다. 이때 cos(θ)는 점점 감소하여 0이 되게 됩니다. θ가 0도에서 90도로 커짐에 따라 녹색선이 줄어들게 된 것이죠. 

cos(θ)는 단위원 내 벡터들끼리의 내적과 같기 때문에 그 내적값이 커진다는 것은 θ가 작아진다(**유사도가 높아진다**)는 의미로 받아들일 수 있습니다. 이러한 사실은 고차원 벡터공간으로도 확대할 수 있으며, Word2Vec 연구진은 이러한 코사인과 내적의 성질을 목적함수 구축에 적극 활용한 것 같습니다. 

그렇다면 위 식 우변을 최대화한다는 말은 어떤 의미를 지니는 걸까요? 분자를 키우고, 분모를 줄이면 최대화 목표를 달성할 수 있겠죠. 우선 분자 부분을 봅시다. 

$$exp({ u }_{ o }^{ T }{ v }_{ c })$$

분자를 증가시킨다는 건 exp의 지수를 크게 한다는 걸 뜻합니다. exp의 지수는 두 벡터의 내적값이 됩니다. 이 값이 커진다는 건 앞서 언급했던 것처럼 벡터들 사이의 θ를 줄인다(즉 유사도를 높인다)는 말이 될 것 같습니다. 다시 말해 중심단어(c)와 주변단어(o)를 벡터공간에 뿌릴 때 인근에 위치시킨다(θ를 줄인다=유사도를 높인다)는 의미로 해석할 수 있다는 얘기입니다.

분모 줄이기는 어떻게 받아들여야 할까요? 분모는 아래와 같습니다.

$$\sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c })}$$

따라서 분모는 중심단어(c)와 학습 말뭉치 내 모든 단어를 각각 내적한 것의 총합입니다. 분모를 줄이려면 주변에 등장하지 않은 단어와 중심단어와의 내적값은 작아져야 합니다. 즉 중심단어 주변에 등장하지 않은 단어에 해당하는 벡터와 중심단어 벡터 사이의 θ를 키운다(**코사인 유사도를 줄인다**)는 의미가 되겠습니다.



## Word2Vec은 단어의 '동시 등장 정보'를 보존한다

여기까지 보면 Word2Vec은 빈도수 기반 방법론들과 별 관련이 없는 것도 같습니다. 그러면 Word2Vec이 어떻게 이와 관계를 지니게 되는 것일까요? 그 비밀은 **학습 과정**에 숨겨져 있습니다.

Word2Vec 네트워크가 p(o\|c)를 키우려면 반드시 정답 셋이 있어야 합니다. '원수는 외나무다리에서 만난다'든지 '나는 학교에 간다'든지 하는 학습용 말뭉치가 있어야 한다는 거죠. 주변단어 몇 개를 볼 지(**window**)를 정해주면 Word2Vec 네트워크는 말뭉치를 window 크기로 슬라이딩하면서 스크린하며 중심단어별로 주변단어들을 보고, p(o\|c)를 키우는 방향으로 각 단어에 해당하는 벡터들의 요소값들을 조금씩 업데이트함으로써 단어를 벡터로 임베딩합니다. 

다시 말해 Word2Vec은 window 내에 등장하지 않는 단어에 해당하는 벡터는 중심단어 벡터와 벡터공간상에서 멀어지게끔(*내적값* **줄이기**), 등장하는 주변단어 벡터는 중심단어 벡터와 가까워지게끔(*내적값* **키우기**) 한다는 것이죠. 

그럼 이렇게 생각해보는건 어떨까요? window 내에 등장하지 않으면 *결과값을* **줄이고**, 등장할 경우 *결과값을* **키우는** 건? 정확히 이 방식으로 작동하는 알고리즘이 오래 전부터 제안돼 왔습니다. 예컨대 주변 단어를 몇 개 볼지를 정하고 동시에 등장하는 단어의 빈도수를 세어서 행렬로 변환한 '[단어-문맥행렬](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/)'이 대표적입니다. 바꿔 말하면 Word2Vec은 기존 count 기반 방법론처럼 자주 같이 등장하는 단어들의 정보(Co-occurrence)를 보존한다는 얘기입니다. [Omer and Yoav(2014)](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)도 Word2Vec은 본질적으로 기존 count 기반의 방법론과 다르지 않다는 점을 논증해 눈길을 끕니다.



## 그렇다면 GloVe, Fasttext는?

[GloVe](http://nlp.stanford.edu/projects/glove/)는 2014년 미국 스탠포드대학 연구팀에서 개발한 단어 임베딩 방법론입니다. GloVe 연구진이 명시적으로 밝혔듯 GloVe가 보존하려는 정보는 단어 동시 등장 여부입니다. GloVe로 임베딩된 단어 벡터끼리의 내적은 동시 등장확률의 로그값과 같습니다(their dot product equals the logarithm of the words' probability of co-occurrence). Word2Vec이 임베딩된 두 단어벡터의 내적이 코사인 유사도라면 GloVe는 동시 등장 확률인 셈이죠. 그럼 GloVe 연구팀이 직접 든 예시를 볼까요?

![GloVe example](http://i.imgur.com/WhWPkMm.png)

위 표의 조건부 확률은 학습 말뭉치에 등장한 빈도수를 기반으로 작성된 것입니다. 예시를 보면 'ice(얼음)'라는 단어가 주어졌을 때 'solid(단단한)'가 동시에 등장할 확률은 'steam(증기)'가 주어졌을 때 'solid'가 나타날 확률보다 높습니다. 반대로 'ice'가 주어졌을 때 'gas(가스)'의 확률은 'steam'일 때 'gas'일 확률보다 낮습니다. 한편 'ice'가 주어졌을 때 'water(물)'이 나올 확률과 'ice'와 'steam'이 동시에 등장할 확률은 비슷합니다. 'fashion'과 'ice', 'fashion'과 'steam' 또한 비슷합니다.

GloVe는 위 표를 기준으로 할 때 P(k\|ice)/P(k\|steam)로 표현되는 **동시 등장 정보 간 비율**을 최대화하고자 합니다. 우선 분자, 분모에 해당하는 확률이 비슷해 분간이 잘 안되는 'water'와 'fashion'은 분석에서 제외합니다. 차이가 있는 'solid', 'gas'가 분석 대상입니다. 비율을 최대화하기 위해선 분자를 키우고, 분모를 줄여야 합니다. 이를 동시에 이해할 필요가 있습니다. 예컨대 'solid'라는 단어를 벡터공간에 임베딩할 때 'ice', 'steam' 중 어느 쪽에 가깝게 둘 것인지 선택하는 문제가 된다는 겁니다. 바꿔 말하면 GloVe는 단어벡터의 내적이 동시 등장 정보 간 비율과 같아지도록 벡터를 임베딩합니다.

페이스북이 2016년 발표한 **[Fasttext](https://research.fb.com/projects/fasttext/)**는 원래 단어를 **부분단어(subword)**의 벡터들로 표현한다는 점을 제외하고는 Word2Vec과 거의 유사합니다. 노이즈가 많은 말뭉치에 강점을 지닌 것으로 알려져 있습니다.





## 세 방법론의 한계

세 방법론이 '단어 동시 등장 정보'를 보존한다는 점에서 기존 '빈도수 세기' 방식과 본질적으로 큰 차이가 없다는 사실을 논의했습니다. 그렇다면 이들 방법론에 한계점은 없는걸까요? 처음에 든 예시를 다시 들어보겠습니다.

> 나는 ___에 간다

위 예시에서 빈칸에는 '학교', '집', '회사' 따위가 모두 들어갈 수 있습니다. 세 방법론은 모두 단어 동시 등장 여부로 단어를 벡터로 바꾸기 때문에 '학교', '집', '회사'라는 단어가 위 예시 문장에서만 쓰였다면 명백히 다른 단어임에도 불구하고 임베딩 벡터 공간에서 세 단어벡터의 유사도가 매우 높게 나타나게 됩니다. 

물론 학습 말뭉치가 충분히 크다면 세 단어가 사용된 사례(문장)이 다양하기 때문에 '학교', '집', '회사' 벡터 간 거리가 충분히 멀게(코사인 유사도가 작게) 임베딩이 될 겁니다. 하지만 사람이 느끼기에 의미적 관련성이 없어 보이는 단어들끼리 매우 유사하게 임베딩되는 사례가 왕왕 있었습니다. 세 방법론이 보존하려는 정보가 '동시등장 여부'뿐이고, '빈도수를 세어' 벡터를 만들기 때문에 생기는 근본적인 한계가 아닐까 하는 생각이 듭니다.



## 마치며

단어 벡터를 만들 때 등장 정보를 보존한다는 점에서는 Word2Vec, GloVe, Fast-text가 모두 기존 count 기반의 방법론과 유사합니다. 다만 동시 등장 정보를 벡터로 표현한 결과(representation)가 **잠재의미분석(Latent Semantic Analysis)** 등 기존 대비 매우 개선됐기 때문에 최근 들어 크게 각광받고 있다고 생각합니다. 포스팅과 관련돼 의견 있으시면 언제든지 댓글, 메일로 주시면 좋을 것 같습니다. 지금까지 읽어주셔서 감사합니다.
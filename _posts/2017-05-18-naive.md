---
title: 나이브 베이지안 분류기 (Naive Bayesian Classifier)
category: Machine Learning
tag: Naive Bayesian Classifer
---

이번 글에서는 문서 분류를 하기 위한 **나이브 베이지안 분류기(Naive Bayesian Classifier)**에 대해 살펴보도록 하겠습니다. 이번 글 역시 고려대 강필성 교수님 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 베이즈 법칙

나이브 베이지안 분류기는 **베이즈 법칙**에 기반한 기법입니다. 일반적으로 사건 $A_1$, $A_2$, $A_3$가 서로 배반(mutually exclusive)이고 $A_1$, $A_2$, $A_3$의 합집합이 표본공간(sample space)과 같으면 사건 $A_1$, $A_2$, $A_3$는 표본공간 $S$의 **분할**이라고 정의합니다. 우리가 관심있는 사건 $B$가 나타날 확률을 그림과 식으로 나타내면 다음과 같습니다.

<a href="http://imgur.com/jC7FfHv"><img src="http://i.imgur.com/jC7FfHv.png" width="400px" title="source: imgur.com" /></a>

$$P(B)=P({ A }_{ 1 }\cap B)+P({ A }_{ 2 }\cap B)+P({ A }_{ 3 }\cap B)$$

P(B)를 조건부확률의 정의를 이용해 다시 쓰면 아래와 같습니다. 이를 **전확률 공식(Law of Total Probability)** 또는 베이즈 법칙이라고 합니다.

$$P(B)=P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 })=\sum _{ i=1 }^{ 3 }{ P({ A }_{ i })P(B|{ A }_{ i }) } $$

보통 $P(A_1)$, $P(A_2)$, $P(A_3)$는 미리 알고 있다는 의미의 **사전확률(prior probability)**로 불립니다. P(B\|A1), P(B\|A2), P(B\|A3)는 **우도(likelihood probability)**라 부릅니다.

그럼 우리가 관심있는 사건인 B가 $A_1$에 기인했을 조건부확률은 어떻게 구할까요? 바로 아래와 같이 구할 수 있습니다. 


$$
\begin{align*}
P({ A }_{ 1 }|B)&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P(B) } \\
&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 }) } 
\end{align*}
$$


P(A1\|B)는 사건 B를 관측한 후에 그 원인이 되는 사건 $A$의 확률을 따졌다는 의미의 **사후확률(posterior probability)**로 정의됩니다. 사후확률은 사건 B의 정보가 더해진, 사전확률의 업데이트 버전 정도라고 생각하면 좋을 것 같습니다. *(Posterior probability is an updated version of prior probability)* 같은 방식으로 P(A2\|B), P(A3\|B)도 구할 수 있습니다. 

한번 예를 들어보겠습니다. 어떤 이름모를 질병에 걸린 환자가 전체 인구의 약 1% 정도 되는 것으로 알려져 있다고 칩시다. 그렇다면 전체 인구라는 표본공간에서 질병에 걸릴 확률 **P(D)**는 0.01, 그렇지 않을 확률 **P(~D)**는 0.99입니다. 이것이 바로 우리가 이미 알고 있는 사전확률이 되겠네요. 

질병 발생 여부를 측정해주는 테스트의 정확도는 이렇다고 합니다. 진짜 환자를 양성이라고 정확하게 진단할 확률 **P(+\|D)**은 97%, 정상환자를 양성이라고 오진할 확률 **P(+\|~D)**은 6%입니다. 이것이 바로 우도입니다.

그럼 진단테스트 결과 양성이라고 나왔는데 실제 환자일 확률 **P(D\|+)**는 얼마일까요? 이건 우리가 이미 알고 있는 정보를 활용해 아래와 같이 구할 수 있습니다. 이것이 바로 사후확률입니다. 즉 '진단'이라는 사건의 정보를 더해 '질병에 걸릴 확률'이라는 사전확률을 업데이트한거죠. 


$$
\begin{align*}
P(+)&=P(D\cap +)+P(\sim  D\cap +)\\ 
&=P(D)P(+|D)+P(\sim  D)P(+|\sim D)\\ 
&=0.01\times 0.97+0.99\times 0.06\\ 
&=0.691\\ P(D|+)&=\frac { P(D)P(+|D) }{ P(+) } \\
&=\frac { 0.01\times 0.97 }{ 0.691 } \\
&=0.014
\end{align*}
$$


그런데 왜 이렇게 복잡하게 사후확률을 구하는거냐고요? 실제로 사후확률은 구하기 어려운 경우가 많다고 합니다. 그에 반해 사전확률은 우리가 이미 알고 있는 값이고, 우도는 비교적 계산하기 수월합니다. 그래서 전확률법칙과 사전확률과 우도를 활용해 사후확률을 도출해내는 것이죠. 

다시 말해 우리가 알고 싶은 확률을 단박에 계산하기가 까다로울 때 조건과 결과를 뒤집어서 우회적으로 계산하는 것, 이것이 베이즈 모델의 강점이라고 말할 수 있겠습니다.



## 나이브 베이즈 모델

문서 이진분류 문제를 예로 들어보겠습니다. 우리가 풀려는 문제는 문서 $d$가 주어졌을 때 범주 $c_1$ 혹은 $c_2$로 분류하는 것입니다. 지금까지 설명한 베이즈 법칙을 다시 쓰면 아래와 같습니다.


$$
\begin{align*}
P({ c }_{ 1 }|d)&=\frac { P({ c }_{ 1 },d) }{ P(d) } =\frac { \frac { P({ c }_{ 1 },d) }{ P({ c }_{ 1 }) } \cdot P({ c }_{ 1 }) }{ P(d) } =\frac { P(d|{ c }_{ 1 }) { P({ c }_{ 1 }) }}{ P(d) } \\ P({ c }_{ 2 }|d)&=\frac { P(d|{ c }_{ 2 }){ P({ c }_{ 2 }) } }{ P(d) }
\end{align*}
$$


위 식에서 $P(c_i)$는 사전확률입니다. 범주 $c_i$인 문서 개수를 전체 문서 개수로 나눈 비율을 뜻합니다. P(d\|$c_i$)는 우도입니다. 문서 $d$인 문서 개수를 범주 $c_i$인 문서 개수로 나눈 비율을 뜻합니다. P($c_i$\|d)는 사후확률입니다. 문서 $d$가 주어졌을 때 해당 문서가 범주 $c_i$일 확률, 즉 우리가 알고 싶은 값입니다.

베이즈 모델은 P($c_1$\|d)와 P($c_2$\|d)를 비교해 큰 쪽으로 범주를 할당합니다. 그런데 여기에서 $P(d)$는 겹치므로 계산을 아예 생략할 수 있습니다. 그러면 위 베이즈 공식을 아래와 같이 다시 쓸 수 있습니다. 


$$
P({ c }_{ i }|d)\propto P(d|{ c }_{ i }){ P({ c }_{ i }) }
$$


만약 문서 범주 비율, 즉 사전확률 $P(c_1)$과 $P(c_2)$가 0.5로 서로 같다면 사전확률 계산도 생략 가능합니다.

$$
P({ c }_{ i }|d)\propto P(d|{ c }_{ i })
$$


이번엔 문서 $d$가 단어 $w_1$, $w_2$로 구성돼 있다고 칩시다. 식을 또 다시 써보겠습니다.


$$
\begin{align*}
P({ c }_{ i }|d)&=P({ c }_{ i }|{ w }_{ 1 },{ w }_{ 2 })\\ &\propto P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i }){ P({ c }_{ i }) } \\ &\propto P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i })
\end{align*}
$$


나이브 베이즈 분류기는 각 단어가 **독립(independent)**임을 가정합니다. 모델 이름에 나이브라는 말이 붙은 이유이기도 합니다. 이에 따라 식을 다시 쓸 수 있습니다.

$$
P({ w }_{ 1 },{ w }_{ 2 })=P({ w }_{ 1 })\cdot P({ w }_{ 2 })\\ P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i })=P({ w }_{ 1 }|{ c }_{ i })\cdot P({ w }_{ 2 }|{ c }_{ i })
$$


## 예시

예를 들어보겠습니다. 'love', 'fantastic' 두 개 단어로 구성된 영화 리뷰1을 긍정, 부정 두 개 범주 가운데 하나로 할당해야 한다고 가정합시다. 리뷰1이 긍정일 확률은 아래와 같이 우도의 연쇄적인 곱으로 구합니다. (긍정/부정 리뷰 비율은 동일하다고 가정)


$$
\begin{align*}
P(positive|{ review }_{ 1 })&\propto P(love|positive)\times P(fantastic|positive)\\ \\ &=\frac { count(love,positive) }{ \sum _{ w\in V }^{  }{ count(w,positive) }  } \times \frac { count(fantastic,positive) }{ \sum _{ w\in V }^{  }{ count(w,positive) }  }
\end{align*}
$$


이와 같은 방식으로 리뷰1이 부정일 확률도 구할 수 있습니다. 둘 중 큰 쪽으로 해당 리뷰의 범주를 할당합니다. 그러면 아래 리뷰 두 개를 분류해 봅시다.

> **review1** : This movie was awesome! I really enjoyed it.
>
> **review2** : This movie was boring and waste of time.

전체 말뭉치로부터 구한 우도는 아래와 같습니다. 

|  Words  | P(Word\|positive) | P(Word\|negative) |
| :-----: | :---------------: | :---------------: |
|  This   |        0.1        |        0.1        |
|  Movie  |        0.1        |        0.1        |
|   Was   |        0.1        |        0.1        |
| Awesome |        0.4        |       0.01        |
|    I    |        0.2        |        0.2        |
| Really  |        0.3        |       0.05        |
| enjoyed |        0.5        |       0.05        |
|   It    |        0.1        |        0.1        |
| Boring  |       0.02        |        0.3        |
|   And   |        0.1        |        0.1        |
|  Waste  |       0.02        |       0.35        |
|   Of    |       0.02        |       0.02        |
|  Time   |       0.15        |       0.15        |

Review1은 위 우도 표에 의해 긍정, Review2는 부정 범주로 분류됩니다.


$$
{ review }_{ 1 }\quad :\quad \prod _{ i }^{  }{ P({ word }_{ i }|Pos)=120\times { 10 }^{ -8 } } >\prod _{ i }^{  }{ P({ word }_{ i }|Neg)=0.5\times { 10 }^{ -8 } } \\ { review }_{ 2 }\quad :\quad \prod _{ i }^{  }{ P({ word }_{ i }|Pos)=0.012\times { 10 }^{ -8 } } <\prod _{ i }^{  }{ P({ word }_{ i }|Neg)=3.15\times { 10 }^{ -8 } }
$$


## 나이브 베이즈 분류기의 장단점

나이브 베이즈 분류기는 앞선 예시의 우도 테이블 하나만 있으면 분류가 가능합니다. 사전확률이 다르다면 전체 문서 범주 비율만 더 반영해주면 됩니다. 그만큼 계산복잡성이 낮다는 얘기입니다. 단어 등장확률을 독립으로 가정하는 [Bag-of-Words](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/) 기법과 잘 어울리는 모델이라고 합니다. 딥러닝 이전 자연언어처리 기법으로 각광받았던 모델입니다.

다만 나이브 베이즈 분류기는 문서에 등장하는 단어 수만큼의 우도 확률 곱으로 분류를 수행하기 때문에 단어 수가 늘어날 수록 그 값이 0으로 수렴하는 경향이 있습니다. 1보다 작은 값은 곱할 수록 작아지는 게 당연한 이치입니다. 게다가 말뭉치에 특정 단어가 등장할 확률은 대단히 낮은 편이라는 단점도 있습니다. 이 때문에 확률값들을 지나치게 작지 않게끔 보정하는 **smoothing** 기법들이 여럿 제안되었습니다.
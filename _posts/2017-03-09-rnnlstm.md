---
title: RNN과 LSTM을 이해해보자!
category: Natural Language Processing
tag: RNN
---

이번 포스팅에서는 **Recurrent Neural Networks(RNN)**과 RNN의 일종인 **Long Short-Term Memory models(LSTM)**에 대해 알아보도록 하겠습니다. 우선 두 알고리즘의 개요를 간략히 언급한 뒤 foward, backward compute pass를 천천히 뜯어보도록 할게요. 이번 포스팅은 기본적으로 미국 스탠포드대학의 [CS231 강좌](http://cs231n.stanford.edu/syllabus.html)를 참고하되 forward, backward pass 관련 설명과 그림은 제가 직접 만들었음을 밝힙니다. 아울러 **Neural Networks**의 기본 구조에 대해선 이미 알고 있다고 가정하고 설명을 하는데요, 혹시 이 부분이 생소하거나 헷갈리시는 분은 홍콩 과기대의 김성훈 교수님 [강의](https://hunkim.github.io/ml/)를 참고하시면 좋을 것 같습니다. 자, 그럼 시작하겠습니다!



## RNN의 기본 구조

![RNN](http://i.imgur.com/Q8zv6TQ.png)

RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류입니다. 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있는데요. **Convolutional Neural Networks(CNN)**과 더불어 최근 들어 각광 받고 있는 알고리즘입니다. 위의 그림에서도 알 수 있듯 길이에 관계없이  인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다. 

![RNN 구조](http://i.imgur.com/s8nYcww.png)

RNN의 기본 구조는 위 그림과 같습니다. 녹색 박스는 히든 state를 의미합니다. 빨간 박스는 인풋(x), 파란 박스는 아웃풋(y)입니다. 현재 상태(t)의 히든 state는 직전 시점(t-1)의 히든 state의 정보를 받아 갱신됩니다. 현재 상태(t)의 아웃풋(y)는 같은 시점의 인풋(x)과 히든 state(h) 정보를 전달받아 갱신되는 구조입니다. 수식에서도 알 수 있듯 히든 state의 **활성함수(activation function)**은 **비선형 함수**인 **하이퍼볼릭탄젠트(tanh)**입니다. 

그런데 활성함수로 왜 비선형 함수를 쓰는걸까요? ['밑바닥부터 시작하는 딥러닝'](http://book.naver.com/bookdb/book_detail.nhn?bid=11492334)의 글귀를 하나 인용해 보겠습니다.

> 선형 함수인 h(x) = cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x) = h(h(h(x)))가 됩니다. 이 계산은 y(x) = c * c * c * x처럼 세번의 곱셈을 수행하지만 실은 y(x) = ax와 똑같은 식입니다. a = c^3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.

RNN의 기본 동작을 직관적으로 이해해 보기 위해 CS231 강좌의 Kapathy~~갓파시~~가 든 예제를 가져와 봤습니다.

![Kapathy의 예제](http://i.imgur.com/vrD0VO1.png)

어떤 글자가 주어졌을 때 바로 다음 글자를 예측하는 character-level의 모델을 만든다고 칩시다. 예컨대 RNN 모델에 'hell'을 넣으면 'o'를 반환하게 해 결과적으로는 'hello'를 출력하게 만들고 싶은 겁니다. 우선 우리가 가진 학습데이터의 글자는 'h', 'e', 'l', 'o' 네 개뿐입니다. 이를 **[one-hot-vector](https://en.wikipedia.org/wiki/One-hot)**로 바꾸면 각각 [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]이 됩니다.

첫번째 단계에서 h에 해당하는 벡터 [1,0,0,0]을 모델에 인풋으로 집어넣습니다. 그걸 기반으로 첫번째 히든state인 [0.3, -0.1, 0.9]를 만들었습니다(첫번째 히든 state를 만들 때 직전 시점의 히든 state는 존재하지 않기 때문에 랜덤 값을 집어넣는 경우가 많습니다). 이 히든 state를 가지고 아웃풋(y)를 [1.0, 2.2, -3.0, 4.1]로 생성했습니다. 마찬가지로 두번째, 세번째, 네번째 단계들도 모두 갱신하게 됩니다. 이 과정을 **foward compute pass**라고 부릅니다.

다른 인공신경망과 마찬가지로 RNN도 정답을 필요로 합니다. 모델에 정답을 알려줘야 모델이 **parameter**를 적절히 갱신해 나가겠죠.  이 경우엔 바로 다음 글자가 정답이 되겠네요. 예컨대 'h'의 다음 정답은 'e', 'e' 다음은 'l', 'l' 다음은 'l', 'l' 다음은 'o'가 정답입니다. 위의 그림을 기준으로 설명을 드리면 첫번째 정답인 'e'는 두번째 요소만 1이고 나머지가 0인 one-hot-vector입니다. 그림을 보면 아웃풋(y) 벡터에 녹색으로 표시된 숫자들이 있는데 이 숫자들이 바로 정답에 해당하는 인덱스값들입니다. 이 정보를 바탕으로 **역전파(backpropagation)**를 수행해 parameter값들을 갱신해 나가는 과정이 바로 RNN의 학습이며 이를 **backward compute pass**라고 부릅니다.

그렇다면 RNN이 학습하는 parameter는 무엇일까요? 그림을 자세히 보시면 알겠지만 인풋(x)을 히든레이어(h)로 보내는 W_xh, 히든레이어(h)에서 히든레이어(h)로 보내는 W_hh, 히든레이어(h)에서 아웃풋(y)으로 보내는 W_hy가 바로 parameter입니다. 그리고 화살표 색깔에서도 알 수 있듯 모든 시점의 state에서 이 parameter는 동일하게 적용됩니다. 





## RNN의 compute pass

앞장에서 말씀드린 RNN의 기본 구조를 토대로 forward compute pass를 아래와 같이 그려봤습니다. 처음엔 어리둥절하겠지만 찬찬히 따라 가다 보면 위에서 설명한 수식을 그래프로 옮겨놓은 것일 뿐입니다. 긴장하지 않으셔도 됩니다.

![RNN의 forward pass](http://i.imgur.com/TIdBDTJ.png)

자, 이제 backward pass를 볼까요? 그 전에 Kapathy가 설명해 놓은 계산그래프를 이용한 역전파 방법에 관한 [포스팅](http://cs231n.github.io/optimization-2/)을 읽어보시길 권합니다. Kapathy 강의의 핵심은 아래 그림과 같습니다.

![backprop](http://i.imgur.com/ttZdVhU.png)

위 그림은 **미분의 연쇄법칙(chain rule)**에 의해 **그래디언트(gradient)**가 전파되는 과정을 도식적으로 나타낸 것입니다. 즉 다음 계산과정으로부터 흘러들어온 그래디언트(dL/dz)에 현재 계산과정(f)에 해당하는 로컬 그래디언트(dz/dx)가 곱해져 이 그래디언트 값(dL/dx)이 이전 계산과정으로 넘어간다는 얘기입니다. 만약 f가 덧셈 연산이라면 다음 계산과정으로부터 흘러들어온 그래디언트(dL/dz)가 바로 이전 계산과정으로 전파가 됩니다**(Gradient distributor)**. f가 곱셈 연산이라면 dL/dx는 dy, dL/dy는 dx가 되는데요, 직접 손으로 써가면서 보게 되면 로컬 그래디언트가 상대방의 변화량으로 바뀌는 모습을 확인할 수 있습니다**(Gradient swticher)**. RNN의 backward compute pass를 시각화하면 아래와 같습니다.

![RNN의 backward pass](http://i.imgur.com/Xtpgxzu.gif)

위 그림을 천천히 따라가 볼까요? 우선 forward pass를 따라 최종 출력되는 결과는 Yt입니다. 최종 Loss(정답과 비교한 손실값)에 대한 Yt의 그래디언트(dL/dYt)가 RNN의 역전파 연산에서 가장 먼저 등장합니다. 표기는 편의상 dYt라고 적었고, 각 연산과정에 해당하는 그래디언트 값은 붉은색으로 표시를 했습니다. dYt는 덧셈 그래프를 타고 양방향에 모두 그대로 분배가 됩니다. 덧셈은 Gradient distributor이기 때문이죠. 따라서 최종 Loss에 대한 by의 그래디언트(dL/dby), 즉 dby는 dYt가 됩니다.

자, 이제 곱셈 쪽(위쪽) 역전파를 볼까요? 덧셈 그래프에서 흘러들어온 그래디언트(dYt)는 곱셈 그래프를 지나 로컬 그래디언트가 곱해지면서 로컬 그래디언트가 뒤바뀌게 됩니다. 곱셈은 Gradient switcher이기 때문이죠. 즉, dWxy는 흘러들어온 그래디언트 dYt에 상대방의 변화량 ht를 곱한 값이 됩니다. 마찬가지로 dht는 흘러들어온 그래디언트 dYt에 Wxy를 곱한 값입니다.

dhraw는 조금 헷갈릴 수 있겠네요. 하지만 기본적인 원리만 알면 어렵지 않습니다. 다시 말해 흘러들어온 그래디언트에 로컬 그래디언트를 곱하면 된다는 거죠. dhraw는 흘러들어온 그래디언트인 dht에 로컬 그래디언트인 1-tanh^2(ht)을 곱하면 됩니다. 하이퍼볼릭탄젠트(tanh(x))의 미분값은 1-tanh^2(x)이기 때문입니다. 이처럼 하이퍼볼릭탄젠트의 미분값은 다른 함수에 비해 구하기가 비교적 쉽기 때문에 활성함수로 많이들 사용하는 것 같습니다.

나머지는 앞 부분과 동일하게 이해하면 됩니다. 앞에서 구한 dhraw는 덧셈 그래프를 타고 각 연산에 그대로 분배가 됩니다. 곱셈 연산에선 흘러들어온 그래디언트에 로컬 그래디언트(상대방의 변화량)가 곱해지게 됩니다.

다만 맨 마지막 컷에 주의할 필요가 있습니다. RNN은 히든 노드가 순환 구조를 띄는 신경망입니다. 즉 직전 시점(t-1)의 히든 state는 현시점(t)의 히든 state를 만드는 데 사용됩니다. 바꿔 말하면 현시점의 dh는 현시점의 Loss에서 흘러들어온 그래디언트뿐 아니라 직전 시점의 dh(즉 Whh * dhraw, 그림에선 별표 표시)가 동시에 반영된다는 뜻입니다. 이것이 RNN이 다른 네트워크와 구조적으로 다른 점 중 하나라고 볼 수 있겠습니다.





## LSTM의 기본 구조

![vanishing grad](http://i.imgur.com/H9UoXdC.png)

RNN은 히든 노드가 순환하는 형태를 가지기 때문에 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이를 **vanishing gradient problem**이라고 합니다. 이런 문제를 극복하기 위해서 고안된 것이 바로 LSTM입니다. LSTM은 RNN의 히든state에 cell-state를 추가한 구조입니다. LSTM을 가장 쉽게 시각화한 [포스트](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)를 기본으로 해서 설명을 이어나가겠습니다.

![RNN vs LSTM](http://i.imgur.com/jKodJ1u.png)

위 그림이 바로 RNN과 LSTM의 차이점을 극명하게 대비해 나타내 줍니다. RNN은 히든 state만 있지만 LSTM엔 여기에 cell state가 추가돼 다소 복잡해졌습니다. cell state의 역할은 그림에서도 알 수 있듯 일종의 컨베이어 벨트 역할을 합니다. 녹색 박스 상단의 라인(cell state)을 보면 곱셈, 덧셈 각 1회씩만 하고 바로 다음 state로 넘어가게 됩니다. 즉 가벼운 선형연산만 이뤄진다는 뜻이죠. 이런 구조에서는 state가 꽤 오래 경과하더라도 그래디언트가 비교적 전파가 잘 되게 됩니다. 그래서 모델 이름에 Long, memory라는 말을 집어넣은 것 같기도 합니다. 자, 그럼 각 게이트가 어떻게 작동하는지 볼까요?

![input&forget gate](http://i.imgur.com/MPb3OvZ.png)

**forget gate**는 말 그대로 '과거 정보를 잊기'를 위한 게이트입니다. 직전 시점(t-1)의 히든 state와 현시점(t)의 인풋(x)을 받아 시그모이드를 취해준 값이 바로 forget gate가 내보내는 값이 됩니다. 아시다시피 시그모이드 함수의 출력 범위는 0에서 1 사이입니다. 즉 forget gate의 값이 0이라면 이전 상태의 정보는 지우고(잊고), 1이라면 이전 상태의 정보를 온전히 전달하게(기억하게) 됩니다. **input gate**는 '현재 정보를 기억하기' 위한 게이트입니다. 직전 시점(t-1)의 히든 state와 현시점(t)의 인풋(x)을 받아 시그모이드와 하이퍼볼릭탄젠트를 각각 취해주고 이를 서로 곱한 값이 바로 input gate가 내보내는 값이 됩니다. 시그모이드와 하이퍼볼릭탄젠트 함수는 현재 정보를 cell에 얼마나 반영할지를 정해주게 됩니다.

![state update](http://i.imgur.com/kKannSP.png)

현시점(t)의 cell state는 직전 시점(t-1)의 cell state에 forget gate 값을 곱하고, input gate가 내보내는 값을 더해 업데이트합니다. 현시점(t)의 히든 state는 직전시점(t-1)의 히든 state와 현시점의 인풋(x)을 받아 시그모이드를 취해준 값에 현시점의 cell state를 곱해 갱신합니다. (Ct를 gt라고 적었는데 이는 CS231의 Kapathy 강의 notation과 일치시키기 위함입니다, 같은 걸 뜻합니다.) 수식에서도 알 수 있듯 cell state는 히든 state에, 히든 state는 cell state에 서로 영향을 주는 구조입니다. 그래디언트를 좀 더 잘 전파하기 위해서이죠. 자, 그럼 지금까지 설명한 LSTM의 forward compute pass를 그래프로 그려보겠습니다.

![LSTM forward pass](http://i.imgur.com/7Jk6szL.png)

여기서 주목해야 할 점은 그래프 중앙에 있는 Ht입니다. 직전 시점(t-1)의 히든 state와 현시점(t)의 인풋(x)을 받아 만든 행렬인데요. RNN과 달리 cell state가 추가됐기 때문에 차원수도 조금 달라지게 됩니다. 다시 말해 다음 상태의 cell state를 만들어 내기 위해선 히든 state 차원수에 해당하는 i, f, o, g 네 개 정보가 필요한데요, 만약 히든 state의 차원수가 *(h x h)*라면 Ht의 차원수는 *(4h x 4h)*가 됩니다. Ht 행렬을 행 기준으로 4등분해 i, f, o, g 각각에 해당하는 활성함수를 적용하는 방식으로 i, f, o, g를 계산하는 것이죠. (물론 이렇게 계산하지 않고 다른 방식을 써도 관계는 없습니다, 행렬 연산을 간편하게 하기 위한 일종의 테크닉이라고 이해하시면 될 것 같습니다.) 이를 그림으로 나타내면 다음과 같습니다.

![Ht의 구조](http://i.imgur.com/73zzDsC.png)

그럼 이제 LSTM의 backward pass를 알아볼까요? 제가 긴 삽질 끝에 만든 움짤을 확인해 보시죠.

![LSTM backward pass](http://i.imgur.com/2BZtc2l.gif)

우선 dft, dit, dgt, dot를 구하기까지 backward pass는 RNN과 본질적으로 다르지 않습니다. dHt를 구하는 과정이 LSTM backward pass 핵심이라고 할 수 있죠. Ht는 it, ft, ot, gt로 구성된 행렬입니다. 바꿔 말하면 각각에 해당하는 그래디언트를 이를 합치면(merge) dHt를 만들 수 있다는 뜻이죠. i, f, o는 활성함수가 시그모이드이고, g만 하이퍼볼릭탄젠트입니다. 시그모이드 함수의 미분식은 주지하다시피 (1-f(x))f(x)이죠. 따라서 dHt의 첫번째 요소는 흘러들어온 그래디언트(dft)에 시그모이드 함수의 로컬 그래디언트((1-ft) * ft)를 곱한 값이 됩니다. 두번째, 세번째 요소도 마찬가지로 구할 수 있습니다. dHt의 네번째 마지막 요소는 흘러들어온 그래디언트(dgt)에 하이퍼볼릭탄젠트 함수의 로컬 그래디언트((1-tanh^2(gt))를 곱해 구할 수 있습니다. 이렇게 구한 dHt는 다시 RNN과 같은 방식으로 역전파가 되는 구조입니다.

RNN과 마찬가지로 맨 마지막 컷에 주목해야 합니다. LSTM은 cell state와 히든 state가 재귀적으로 구해지는 네트워크입니다. 따라서 cell state의 그래디언트와 히든 state의 그래디언트는 직전 시점의 그래디언트 값에 영향을 받습니다. 이를 역전파시 반영해야 합니다.



## 파이썬 구현

지금까지 RNN, LSTM 구조에 대해 알아봤습니다. 구조에 대해 빡세게 공부했으니 이제 코드로 구현해야 할 일만 남았죠. CS231 강좌의 Kapathy가 간단한 구조의 RNN에 대해선 파이썬 numpy 패키지만 활용해 구현해놓은 [코드](https://gist.github.com/karpathy/d4dee566867f8291f086)가 있습니다. 여기서 손실함수만 바꾸면 비교적 쉽게 LSTM 구조로 변경할 수가 있는데요. 제가 인터넷에 떠다니는 여러 자료를 보면서 글자나 단어 단위로 학습하기 위한 LSTM 손실 함수를 만들어봤습니다. 위 설명과 notation이 약간 다르긴 한데, 본질적으로는 완전히 같은 코드입니다.

<script src="https://gist.github.com/ratsgo/6e9a094c7108dee8147ef0a13666de47.js"></script>

위 손실함수에서 **dy[targets[a]] -= 1**에 주목할 필요가 있습니다. 역전파의 시작을 알리는 코드인데요. 주지하다시피 RNN이든 LSTM이든 맨 마지막에 소프트맥스 확률(y)을 구하고 이를 통해 **Cross Entropy Error**를 구하게 됩니다. 이 둘을 결합한 레이어를 보통 **Softmax-with-Loss**라고 부르는데요. 정답이 3개 클래스(t1, t2, t3)로 이뤄져 있다고 가정할 때 이 레이어의 역전파는 정확히 (y1-t1, y2-t2, y3-t3)이 됩니다. 그런데 앞서 설명드렸듯 우리는 글자를 one-hot-vec로 변환했기 때문에 정답에 해당하는 인덱스 값만 1이고 나머지는 모두 0이 됩니다. 만약 t1이 정답이라고 치면, 역전파는 (y1-1, y2, y3)가 된다는 뜻입니다. dy[targets[a]] -= 1은 정확히 이걸 구현하는 코드라고 보시면 되겠습니다. 혹시 자세한 내용을 보시려면 ['밑바닥부터 시작하는 딥러닝'](https://books.google.co.kr/books?id=rjLyDQAAQBAJ&pg=PA291&lpg=PA291&dq=%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4+%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84&source=bl&ots=maUenMv-if&sig=qG4bgNCgRo9E7R55IyF9hl04cuo&hl=ko&sa=X&ved=0ahUKEwjfyqe20cjSAhWCj5QKHc6cD3QQ6AEIJDAB#v=onepage&q=%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84&f=false)을 참고하시면 좋을 것 같습니다.





## 파일럿 실험

꽤 오랜 시간 공들여 삽질한 결과물인데, 어디에든 써먹어보자는 생각이 들었습니다. 그래서 택한 데이터가 이광수 장편소설 '무정'이었는데요. 32만 어절로 이뤄진 작품입니다. 1917년 작품이라 한자어가 많고 대화체 문장이 많습니다. 예컨대 텍스트는 이렇게 생겼습니다.

```
형식은, 아뿔싸! 내가 어찌하여 이러한 생각을 하는가, 내 마음이 이렇게 약하던가 하면서 두 주먹을 불끈 쥐고 전신에 힘을 주어 이러한 약한 생각을 떼어 버리려 하나, 가슴속에는 이상하게 불길이 확확 일어난다. 이때에, “미스터 리, 어디로가는가” 하는 소리에 깜짝 놀라 고개를 들었다. (중략) 형식은 얼마큼 마음에 수치한 생각이 나서 고개를 돌리며, “아직 그런말에 익숙지를 못해서……” 하고 말끝을 못 맺는다. 
“대관절 어디로 가는 길인가? 급하지 않거든 점심이나 하세그려.”
“점심은 먹었는걸.”
“그러면 맥주나 한잔 먹지.”
“내가 술을 먹는가.”
(중략)
“요― 오메데토오(아― 축하하네). 이이나즈케(약혼한사람)가 있나 보네 그려. 음나루호도(그러려니). 그러구도 내게는 아무 말도 없단 말이야. 에, 여보게”하고 손을 후려친다.
```
이 텍스트를 글자 단위로 one-hot-vector로 바꾼 뒤 LSTM에 넣어 학습시켜 보기로 했습니다. 하이퍼파라메터는 히든 차원수 100, learning rate 0.1을 줬습니다.  다음은 학습 결과입니다.

> Iter 0 : 랫萬게좁뉘쁠름끈玄른작밭裸觀갈나맡文플조바늠헝伍下잊볕홀툽뤘혈調記운피悲렙司狼독벗칼둡걷착날完잣老엇낫業4改‘촉수릎낯깽잊쯤죽道넌友련친씌았융타雲채發造거크휘탁亨律與命텐암먼헝평琵헤落유리벤産이馨텐 

> Iter 4900: 를왔다내 루방덩이종 은얼에는 집어흔영채는아무  우선을 에서가며 건들하아버전는 애양을자에  운 모양이 랐다. 은 한다선과 ‘마는 .식세식가들어 ,
>
> 형식다 
>
> “내었다.있이문

> Iter750000 :  으로 유안하였다. 더할까하는 세상이 솔이요, 알고 게식도 들어울는 듯하였다. 태에그려 깔깔고 웃는듯이 흔반다. 우선형은사람을 어려보낸다.
>
> “그려가?”
>
> 한다. 영채는손을 기쁘 

> Iter1000000 :  에 돌내면서,
>
> “여러 넣어오습데다. 그 말대 아무도좀 집림과 시오 백매, 저는 열녀더러, 기런 소년이가아니라.”
>
> “어리지요.”
>
> 노파도 놀라며,
>
> “저희마다가말없습니까.”
>
> “아니 (대화체)

꽤 오랜 시간 학습시켰음에도 여전히 뜻 모를 글자들을 내뱉고 있는 점이 아쉽습니다. 다만 '영채', '선형' 등 무정 인물명들을 언급하거나, 따옴표를 써서 대화체 문장을 구성하거나, '-요' '-까' '-다' 같은 종결어미를 사용해 문장을 끝맺고 있는 등 잘 하고 있는 점도 눈에 띕니다. 지금은 그저 공부할 목적으로 제일 간단한 구조로 학습을 시켰는데, 구조나 인풋 등 미세 조정을 하게 된다면 한국어 분석에서도 LSTM이 활약할 여지가 충분할 것 같다는 생각이 듭니다.

지금까지 RNN과 LSTM에 대해 알아봤습니다. 이 포스팅은 제가 공부할 목적으로 작성한 만큼 오류나 개선사항이 있을 수 있습니다. 이메일이나 댓글로 언제든 편하게 말씀주시면 제게 큰 기쁨이 될 것 같습니다. 읽어주셔서 감사합니다.
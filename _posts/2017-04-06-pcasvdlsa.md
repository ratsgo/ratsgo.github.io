---
title: SVD와 PCA, 그리고 잠재의미분석(LSA)
category: From frequency to semantics
tag: SVD, PCA, LSA
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

이번 포스팅에서는 **차원축소(dimension reduction)** 기법으로 널리 쓰이고 있는 **특이값분해(Singular Value Decomposion)**와 **주성분분석(Principal Component Analysis)**에 대해 알아보도록 하겠습니다. 마지막으로는 이러한 기법이 **잠재의미분석(Latent Sematic Analysis)**와 어떻게 연결되는지 이야기해보도록 하겠습니다. 이번 글은 고려대 강필성 교수님, 한양대 이상화 교수님 강의와 [이곳](https://www.quora.com/What-is-an-intuitive-explanation-of-the-relation-between-PCA-and-SVD)을 참고했음을 미리 밝힙니다. 그럼 시작하겠습니다.



## 주성분 분석

PCA는 데이터의 **분산(variance)**을 최대한 보존하는 서로 직교하는 새로운 기저(축)를 찾아, 서로 연관가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다. 이를 그림으로 나타내면 아래와 같습니다. 3차원 공간에 있는 데이터들이 서로 수직인 두 개의 주성분(PC1, PC2)을 새로운 기저로, 선형변환된 것을 확인할 수 있습니다.

<a href="http://imgur.com/jWJ2nUs"><img src="http://i.imgur.com/jWJ2nUs.png" width="600px" title="source: imgur.com" /></a>

원 데이터의 분산을 최대화하는 새로운 기저를 찾는 목표를 달성하려면 우선 데이터의 공분산 행렬부터 구해야 합니다. 데이터가 각 변수별로 평균이 0으로 맞춰져 있을 때(centering 작업 이미 수행되어 있다고 가정) 공분산 행렬은 아래와 같이 구합니다.

$$cov(M)=\frac { 1 }{ n-1 } { M }^{ T }M\propto { M }^{ T }M$$

만일 M이 m x n 크기의 행렬이라면 공분산 행렬은 n x n 크기의 **정방행렬(square matrix)**이 됩니다.  또한 공분산 행렬은 그 전치행렬과 동일한 **대칭행렬(symmetric matrix)**이며, 모든 고유값이 음수가 아닌 **비음수행렬(positive-semidefite matrix)**입니다. PCA의 새로운 축을 찾기 위해서는 위 공분산행렬을 아래처럼 **고유분해(Eigen decomposition)**를 수행해주어야 합니다. 아래 식에서 Λ는 대각성분이 공분산행렬의 고유값인 고유값행렬, V는 열벡터가 공분산행렬의 고유벡터로 이뤄진 고유벡터행렬입니다. 

$${ M }^{ T }M=V\Lambda { V }^{ -1 }$$

여기에서 Λ의 대각성분은 데이터 행렬 M의 각 변수에 해당하는 분산을 의미합니다. 원 데이터의 분산을 최대화하는 새로운 기저를 찾는 것이 PCA 목표인 만큼 가장 큰 고유값 몇 개를 고르고, 그에 해당하는 고유벡터를 찾아 이 벡터를 새로운 기저로 원데이터를 선형변환해주면 PCA 작업이 완료되게 됩니다. 예컨대 변수가 100개(100차원)인 데이터에 PCA를 적용한 후 가장 큰 고유값 두 개를 골라 그에 해당하는 고유벡터로 원 데이터를 사영시키면 데이터의 차원수를 100차원에서 2차원으로 줄일 수 있게 됩니다. 아울러 공분산행렬이 비음수행렬인 사실과 Λ의 대각성분이 각 변수의 분산(편차의 제곱, 그러므로 항상 0 이상)이라는 점도 재미있는 부분인 것 같습니다.

그런데 여기서 공분산행렬은 대칭행렬이라는 점에 주목할 필요가 있습니다. 대칭행렬을 위와 같이 고유분해를 하게 되면 V는 **직교행렬(orthogonal matrix)**이 됩니다. 다시 말해 V에 속한 고유벡터들은 PCA로 찾아진 새로운 주성분, 즉 기저이며 이들이 서로 **직교(orthogonal)**한다는 이야기입니다. 이 축에 원 데이터를 **사영(projection)**시키면 PCA 작업이 완료가 되는데요. 사영 전에는 변수 간 연관성이 있었더라도 PCA 변환에 의하여 좌표축이 바뀐 데이터들은 서로 **무상관(uncorrelated)**이게 됩니다. 대칭행렬은 그 전치와 역행렬이 같으므로 V와 공분산행렬은 아래와 같이 쓸 수 있습니다.

$${ V }^{ T }{ V }={ V }^{ -1 }V=I\\ { M }^{ T }M=V\Lambda { V }^{ T }$$



## 특이값 분해

특이값분해는 임의의 m x n 크기 행렬 M을 아래와 같이 분해하는 걸 말합니다. 행렬 U와 V에 속한 벡터는 **특이벡터(singular vector)**로 불리고요, 모든 특이벡터는 서로 직교하는 성질을 지닙니다. U와 V의 특이벡터는 각각 M^t\*M, M\*M^t의 고유벡터들입니다. 또 행렬 Σ의 특이값은 모두 0보다 크거나 같으며 내림차순으로 정렬돼 있습니다. 행렬 Σ의 i번째 대각원소에 해당하는 Si는 행렬 M의 i번째 고유값에 제곱근을 취한 값과 같습니다.

<a href="http://imgur.com/3Es8n8L"><img src="http://i.imgur.com/3Es8n8L.png" width="500px" title="source: imgur.com" /></a>

$$M=U\Sigma { V }^{ T }\\ { U }^{ T }U={ V }^{ T }V=I\\M\overrightarrow { { v }_{ i } } ={ \sigma  }_{ i }\overrightarrow { { u }_{ i } } \\ { \sigma  }_{ i }=\sqrt { { \lambda  }_{ i } } $$



그러면 특이값 분해를 주성분 분석과 비교해보기 위해 행렬 M을 두번 곱해 보겠습니다. 이후 식을 정리하면 아래와 같습니다.
$$
\begin{align*}
{ M }^{ T }M&={ (U\Sigma { V }^{ T }) }^{ T }U\Sigma { V }^{ T }\\ 
&=V\Sigma { U }^{ T }U\Sigma { V }^{ T }\\ &=V{ \Sigma  }^{ 2 }{ V }^{ T }\\
&=V\Lambda { V }^{ T }
\end{align*}
$$

여기서 대각성분이 행렬 M의 특이값인 Σ에 주의할 필요가 있습니다. Σ는 **대각행렬(diagonal matrix)**인데요, 대각행렬의 거듭제곱은 대각원소들만 거듭제곱을 해주면 됩니다. 따라서 Σ의 제곱은 각 대각원소, 즉 행렬 M의 특이값들을 제곱해준 값과 똑같습니다. 그런데 행렬 M의 특이값은 같은 행렬의 고유값에 제곱근을 취한 값과 동일하므로, Σ을 제곱한 행렬은 행렬 M의 고유값으로 이뤄진 행렬 Λ입니다. 이는 정확히 주성분 분석의 결과와 같습니다.

그렇다면 주성분 분석과 특이값 분해 가운데 어떤 기법이 나을까요? 둘의 결과는 사실상 동일하지만 계산복잡성을 생각하면 특이값 분해가 조금 낫다고 합니다. 특이값 분해는 임의의 사각행렬에 모두 바로 적용 가능하지만 주성분 분석의 경우 데이터 행렬의 공분산 행렬을 구해야 하기 때문입니다.



## 잠재의미분석

잠재의미분석이란 **단어-문서행렬(Word-Document Matrix)**, **단어-문맥행렬(window based co-occurrence matrix)** 등입력 데이터에 특이값분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론입니다. 대략적인 개념은 아래 그림과 같습니다.

<a href="http://imgur.com/zsGpFOd"><img src="http://i.imgur.com/zsGpFOd.png" width="600px" title="source: imgur.com" /></a>

우선 n개의 문서를 m개의 단어로 표현된 입력데이터 행렬 M이 주어졌다고 칩시다. 자연언어처리에서 입력데이터에 잠재의미분석을 수행하는 절차는 이렇습니다. 입력데이터 행렬 M의 0보다 큰 고유값의 개수를 r이라고 할 때, r보다 작은 k를 연구자가 임의로 설정합니다. 이렇게 만든 Mk에 Uk의 전치행렬을 곱해줍니다. 그러면 n개의 문서는 원래 단어수 m보다 훨씬 작은 k개 변수로 표현할 수가 있게 됩니다. 이는 주성분 분석에서의 차원축소 효과와 비슷한 것으로 이해하면 좋을 것 같습니다. 이렇게 만든 X에 다양한 데이터마이닝 기법을 적용해 여러 가지 문제를 풀게 되는 것입니다.

$${ M }_{ k }={ U }_{ k }{ \Sigma  }_{ k }{ V }_{ k }^{ T }\\ { { U }_{ k }^{ T }M }_{ k }={ \Sigma  }_{ k }{ V }_{ k }^{ T }=X$$

잠재의미분석은 데이터 차원 축소 이외에도 많은 장점이 있다고 합니다. Deerwester et ak.(1990)과 Landauer and Dumais(1997)은 이 기법을 적용하면 단어와 문맥 간의 내재적인 의미(latent/hidden meaning)을 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 합니다. Rapp(2003)은 입력 데이터의 노이즈 제거, Vozalis and Margaritis(2003)은 입력데이터의 **sparsity**를 줄이게 돼 그 효과가 좋다고 합니다. 다만 SVD를 기반으로 한 잠재의미분석은 입력데이터의 크기가 m x n일 경우 계산복잡도가 O(mn^2)이나 돼 매우 높습니다. 게다가 새로운 문서나 단어가 추가되면 아예 처음부터 작업을 새로 시작해야 합니다. 이 때문에 최근에는 Word2Vec 등 뉴럴네트워크 기반의 representation 방법론이 각광을 받고 있습니다.

